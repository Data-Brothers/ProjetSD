
@misc{noauthor_how_nodate,
	title = {How do {I} know the current version of pytorch? - vision - {PyTorch} {Forums}},
	url = {https://discuss.pytorch.org/t/how-do-i-know-the-current-version-of-pytorch/6754},
	urldate = {2020-11-10},
	file = {How do I know the current version of pytorch? - vision - PyTorch Forums:/home/pierre/snap/zotero-snap/common/Zotero/storage/Q6893ETS/6754.html:text/html},
}

@misc{noauthor_bert_nodate,
	title = {{BERT} (mod√®le de langage) ‚Äî {Wikip√©dia}},
	url = {https://fr.wikipedia.org/wiki/BERT_(mod%C3%A8le_de_langage)},
	urldate = {2020-11-10},
	file = {BERT (mod√®le de langage) ‚Äî Wikip√©dia:/home/pierre/snap/zotero-snap/common/Zotero/storage/BL6QYHUG/BERT_(mod√®le_de_langage).html:text/html},
}

@article{castera_inertial_2020,
	title = {An {Inertial} {Newton} {Algorithm} for {Deep} {Learning}},
	url = {http://arxiv.org/abs/1905.12278},
	abstract = {We introduce a new second-order inertial optimization method for machine learning called INDIAN. It exploits the geometry of the loss function while only requiring stochastic approximations of the function values and the generalized gradients. This makes INDIAN fully implementable and adapted to large-scale optimization problems such as the training of deep neural networks. The algorithm combines both gradient-descent and Newton-like behaviors as well as inertia. We prove the convergence of INDIAN for most deep learning problems. To do so, we provide a well-suited framework to analyze deep learning loss functions involving tame optimization in which we study the continuous dynamical system together with the discrete stochastic approximations. We prove sublinear convergence for the continuous-time diÔ¨Äerential inclusion which underlies our algorithm. Besides, we also show how standard optimization mini-batch methods applied to nonsmooth nonconvex problems can yield a certain type of spurious stationary points never discussed before. We address this issue by providing a theoretical framework around the new idea of D-criticality; we then give a simple asymptotic analysis of INDIAN. Our algorithm allows for using an aggressive learning rate of o(1/ log k). From an empirical viewpoint, we show that INDIAN returns competitive results with respect to state of the art (stochastic gradient descent, ADAGRAD, ADAM) on popular deep learning benchmark problems.},
	language = {en},
	urldate = {2020-11-20},
	journal = {arXiv:1905.12278 [cs, math, stat]},
	author = {Castera, Camille and Bolte, J√©r√¥me and F√©votte, C√©dric and Pauwels, Edouard},
	month = oct,
	year = {2020},
	note = {arXiv: 1905.12278},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
	file = {Castera et al. - 2020 - An Inertial Newton Algorithm for Deep Learning.pdf:/home/pierre/snap/zotero-snap/common/Zotero/storage/FS69P4JL/Castera et al. - 2020 - An Inertial Newton Algorithm for Deep Learning.pdf:application/pdf},
}

@misc{noauthor_indian-for-deeplearningindian_for_tensorflow_nodate,
	title = {Indian-for-{DeepLearning}/indian\_for\_tensorflow at master ¬∑ camcastera/{Indian}-for-{DeepLearning}},
	url = {https://github.com/camcastera/Indian-for-DeepLearning/tree/master/indian_for_tensorflow},
	urldate = {2020-11-20},
	file = {Indian-for-DeepLearning/indian_for_tensorflow at master ¬∑ camcastera/Indian-for-DeepLearning:/home/pierre/snap/zotero-snap/common/Zotero/storage/MGLNK6TT/indian_for_tensorflow.html:text/html},
}

@misc{noauthor_illustrated_nodate,
	title = {The {Illustrated} {Transformer} ‚Äì {Jay} {Alammar} ‚Äì {Visualizing} machine learning one concept at a time.},
	url = {http://jalammar.github.io/illustrated-transformer/},
	urldate = {2020-11-20},
	file = {The Illustrated Transformer ‚Äì Jay Alammar ‚Äì Visualizing machine learning one concept at a time.:/home/pierre/snap/zotero-snap/common/Zotero/storage/P4PAQIVI/illustrated-transformer.html:text/html},
}

@misc{noauthor__nodate,
	title = {üèé {Smaller}, faster, cheaper, lighter: {Introducing} {DistilBERT}, a distilled version of {BERT} {\textbar} by {Victor} {Sanh} {\textbar} {HuggingFace} {\textbar} {Medium}},
	url = {https://medium.com/huggingface/distilbert-8cf3380435b5},
	urldate = {2020-11-20},
	file = {üèé Smaller, faster, cheaper, lighter\: Introducing DistilBERT, a distilled version of BERT | by Victor Sanh | HuggingFace | Medium:/home/pierre/snap/zotero-snap/common/Zotero/storage/C4A2SXJP/distilbert-8cf3380435b5.html:text/html},
}

@article{sanh_distilbert_2020,
	title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	shorttitle = {{DistilBERT}, a distilled version of {BERT}},
	url = {http://arxiv.org/abs/1910.01108},
	abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be Ô¨Ånetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-speciÔ¨Åc models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
	language = {en},
	urldate = {2020-11-20},
	journal = {arXiv:1910.01108 [cs]},
	author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	month = feb,
	year = {2020},
	note = {arXiv: 1910.01108},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: February 2020 - Revision: fix bug in evaluation metrics, updated metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS 2019},
	file = {Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf:/home/pierre/snap/zotero-snap/common/Zotero/storage/3X474IFX/Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf:application/pdf},
}

@misc{desarda_working_2020,
	title = {Working with {Hugging} {Face} {Transformers} and {TF} 2.0},
	url = {https://towardsdatascience.com/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a},
	abstract = {Models based on Transformers are the current sensation in the world of NLP. Hugging Face‚Äôs Transformers library provides all SOTA model‚Ä¶},
	language = {en},
	urldate = {2020-11-22},
	journal = {Medium},
	author = {Desarda, Akash},
	month = apr,
	year = {2020},
	file = {Snapshot:/home/pierre/snap/zotero-snap/common/Zotero/storage/NP59PADV/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a.html:text/html},
}

@article{giorgi_declutr_2020,
	title = {{DeCLUTR}: {Deep} {Contrastive} {Learning} for {Unsupervised} {Textual} {Representations}},
	shorttitle = {{DeCLUTR}},
	url = {http://arxiv.org/abs/2006.03659},
	abstract = {We present DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations, a self-supervised method for learning universal sentence embeddings that transfer to a wide variety of natural language processing (NLP) tasks. Our objective leverages recent advances in deep metric learning (DML) and has the advantage of being conceptually simple and easy to implement, requiring no specialized architectures or labelled training data. We demonstrate that our objective can be used to pretrain transformers to state-of-the-art performance on SentEval, a popular benchmark for evaluating universal sentence embeddings, outperforming existing supervised, semi-supervised and unsupervised methods. We perform extensive ablations to determine which factors contribute to the quality of the learned embeddings. Our code will be publicly available and can be easily adapted to new datasets or used to embed unseen text.},
	urldate = {2020-12-10},
	journal = {arXiv:2006.03659 [cs]},
	author = {Giorgi, John M. and Nitski, Osvald and Bader, Gary D. and Wang, Bo},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.03659},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/pierre/snap/zotero-snap/common/Zotero/storage/SUDU6GR5/Giorgi et al. - 2020 - DeCLUTR Deep Contrastive Learning for Unsupervise.pdf:application/pdf;arXiv.org Snapshot:/home/pierre/snap/zotero-snap/common/Zotero/storage/PU5NSFRL/2006.html:text/html},
}

@article{wu_comprehensive_2020,
	title = {A {Comprehensive} {Survey} on {Graph} {Neural} {Networks}},
	issn = {2162-237X, 2162-2388},
	url = {http://arxiv.org/abs/1901.00596},
	doi = {10.1109/TNNLS.2020.2978386},
	abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classiÔ¨Åcation and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed signiÔ¨Åcant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning Ô¨Åelds. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing Ô¨Åeld.},
	language = {en},
	urldate = {2020-12-10},
	journal = {IEEE Trans. Neural Netw. Learning Syst.},
	author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
	year = {2020},
	note = {arXiv: 1901.00596},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1--21},
	annote = {Comment: Minor revision (updated tables and references)},
	file = {Wu et al. - 2020 - A Comprehensive Survey on Graph Neural Networks.pdf:/home/pierre/snap/zotero-snap/common/Zotero/storage/3MTJEJ7V/Wu et al. - 2020 - A Comprehensive Survey on Graph Neural Networks.pdf:application/pdf},
}

@article{khosla_supervised_2020,
	title = {Supervised {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2004.11362},
	abstract = {Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or signiÔ¨Åcantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4\% on the ImageNet dataset, which is 0.8\% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows beneÔ¨Åts for robustness to natural corruptions, and is more stable to hyperparameter settings such as optimizers and data augmentations. Our loss function is simple to implement and reference TensorFlow code is released at https://t.ly/supcon 1.},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:2004.11362 [cs, stat]},
	author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	month = nov,
	year = {2020},
	note = {arXiv: 2004.11362},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Khosla et al. - 2020 - Supervised Contrastive Learning.pdf:/home/pierre/snap/zotero-snap/common/Zotero/storage/URTDKCRT/Khosla et al. - 2020 - Supervised Contrastive Learning.pdf:application/pdf},
}

@article{baek_rethinking_2020,
	title = {Rethinking the {Truly} {Unsupervised} {Image}-to-{Image} {Translation}},
	url = {http://arxiv.org/abs/2006.06500},
	abstract = {Every recent image-to-image translation model uses either image-level (i.e. input-output pairs) or set-level (i.e. domain labels) supervision at minimum. However, even the set-level supervision can be a serious bottleneck for data collection in practice. In this paper, we tackle image-to-image translation in a fully unsupervised setting, i.e., neither paired images nor domain labels. To this end, we propose the truly unsupervised image-to-image translation method (TUNIT) that simultaneously learns to separate image domains via an information-theoretic approach and generate corresponding images using the estimated domain labels. Experimental results on various datasets show that the proposed method successfully separates domains and translates images across those domains. In addition, our model outperforms existing set-level supervised methods under a semi-supervised setting, where a subset of domain labels is provided. The source code is available at https://github.com/clovaai/tunit},
	language = {en},
	urldate = {2020-12-10},
	journal = {arXiv:2006.06500 [cs]},
	author = {Baek, Kyungjune and Choi, Yunjey and Uh, Youngjung and Yoo, Jaejun and Shim, Hyunjung},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.06500},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Baek et al. - 2020 - Rethinking the Truly Unsupervised Image-to-Image T.pdf:/home/pierre/snap/zotero-snap/common/Zotero/storage/RRBUUYHN/Baek et al. - 2020 - Rethinking the Truly Unsupervised Image-to-Image T.pdf:application/pdf},
}

@inproceedings{croce_gan-bert_2020,
	address = {Online},
	title = {{GAN}-{BERT}: {Generative} {Adversarial} {Learning} for {Robust} {Text} {Classification} with a {Bunch} of {Labeled} {Examples}},
	shorttitle = {{GAN}-{BERT}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.191},
	doi = {10.18653/v1/2020.acl-main.191},
	abstract = {Recent Transformer-based architectures, e.g., BERT, provide impressive results in many Natural Language Processing tasks. However, most of the adopted benchmarks are made of (sometimes hundreds of) thousands of examples. In many real scenarios, obtaining highquality annotated data is expensive and timeconsuming; in contrast, unlabeled examples characterizing the target task can be, in general, easily collected. One promising method to enable semi-supervised learning has been proposed in image processing, based on SemiSupervised Generative Adversarial Networks. In this paper, we propose GAN-BERT that extends the Ô¨Åne-tuning of BERT-like architectures with unlabeled data in a generative adversarial setting. Experimental results show that the requirement for annotated examples can be drastically reduced (up to only 50-100 annotated examples), still obtaining good performances in several sentence classiÔ¨Åcation tasks.},
	language = {en},
	urldate = {2020-12-10},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Croce, Danilo and Castellucci, Giuseppe and Basili, Roberto},
	year = {2020},
	pages = {2114--2119},
	file = {Croce et al. - 2020 - GAN-BERT Generative Adversarial Learning for Robu.pdf:/home/pierre/snap/zotero-snap/common/Zotero/storage/RV9BEZEE/Croce et al. - 2020 - GAN-BERT Generative Adversarial Learning for Robu.pdf:application/pdf},
}

@misc{alammar_illustrated_nodate,
	title = {The {Illustrated} {BERT}, {ELMo}, and co. ({How} {NLP} {Cracked} {Transfer} {Learning})},
	url = {http://jalammar.github.io/illustrated-bert/},
	abstract = {Discussions:
Hacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)


Translations: Chinese (Simplified), French, Japanese, Korean, Persian, Russian

The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (It‚Äôs been referred to as NLP‚Äôs ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).},
	urldate = {2020-12-30},
	author = {Alammar, Jay},
	file = {Snapshot:/home/pierre/snap/zotero-snap/common/Zotero/storage/XGJBMCP2/illustrated-bert.html:text/html},
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be Ô¨Ånetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciÔ¨Åc architecture modiÔ¨Åcations.},
	language = {en},
	urldate = {2020-12-30},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:/home/pierre/snap/zotero-snap/common/Zotero/storage/QMNG359D/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@misc{bourdois_laugmentation_2020,
	title = {L‚Äô{AUGMENTATION} {DE} {DONNEES} {EN} {NLP}},
	url = {https://lbourdois.github.io/blog/nlp/Data-augmentation-in-NLP/},
	abstract = {NLP - Un aper√ßu des techniques disponibles pour r√©aliser de l‚Äôaugmentation de donn√©es textuelles en traitement du langage naturel},
	language = {fr},
	urldate = {2020-12-31},
	journal = {Lo√Øck BOURDOIS},
	translator = {BOURDOIS, Lo√Øck},
	month = may,
	year = {2020},
	file = {Snapshot:/home/pierre/snap/zotero-snap/common/Zotero/storage/6EAATQTF/Data-augmentation-in-NLP.html:text/html},
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiÔ¨Åcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2020-12-30},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 15 pages, 5 figures},
	file = {Vaswani et al. - 2017 - Attention Is All You Need.pdf:/home/pierre/snap/zotero-snap/common/Zotero/storage/N96YDHMF/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf},
}
