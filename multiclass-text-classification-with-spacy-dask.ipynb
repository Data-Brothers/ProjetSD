{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Food Reviews Analysis using Spacy & Dask\n",
    "\n",
    "## Purpose\n",
    "The purpose of this kernel is to illustrate the application of Dask and Spacy for Multiclass Text classification problem where the classes are unbalanced. \n",
    "\n",
    "## Methodology\n",
    "Setup the Dask distributed to handle the text preprocessing and model building in parallel. The model utilizes spacy tokenizer, Hashing vectorizer for text preprocessing. For model building SGD classifier of scikit learn is then used within Incremental wrapper of Dask framework.\n",
    "\n",
    "\n",
    "## Improvements\n",
    "- Need to find better ways to partition the data for faster analysis;\n",
    "\n",
    "## Results\n",
    "\n",
    "1. Mix of Unigram + Bigram with Class Weights\n",
    "    \n",
    "       Class    precision    recall  f1-score   support\n",
    "\n",
    "         1       0.58      0.69      0.63     10327\n",
    "         2       0.35      0.45      0.39      6000\n",
    "         3       0.39      0.43      0.41      8638\n",
    "         4       0.53      0.29      0.38     16028\n",
    "         5       0.85      0.88      0.86     72748\n",
    " \n",
    "                    \n",
    "  - accuracy       0.72\n",
    "\n",
    "\n",
    "## Suggested next steps\n",
    "- More extensive text clean up can be done using spacy with better exploratory analysis of text;\n",
    "- Certain fields within the Dataset where ignored which can be incorporated for better accuracy;\n",
    "- Hyperparameter Tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/amazon-fine-food-reviews/database.sqlite\n",
      "/kaggle/input/amazon-fine-food-reviews/hashes.txt\n",
      "/kaggle/input/amazon-fine-food-reviews/Reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "PATH = '/kaggle/input/amazon-fine-food-reviews/Reviews.csv'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import spacy\n",
    "import pickle\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.feature_extraction.text import HashingVectorizer\n",
    "from dask_ml.wrappers import Incremental\n",
    "from dask_ml.metrics import accuracy_score\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:45455</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>5</li>\n",
       "  <li><b>Cores: </b>10</li>\n",
       "  <li><b>Memory: </b>15.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:45455' processes=5 threads=10, memory=15.00 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(threads_per_worker=2,\n",
    "                n_workers=5, memory_limit='3GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqd = ['Text', 'Score']\n",
    "Reviews_df = dd.read_csv(PATH,\n",
    "                         usecols = reqd,\n",
    "                         blocksize=20e6,\n",
    "                         dtype={'Score': 'float'},\n",
    "                         engine='python',\n",
    "                         encoding='utf-8',\n",
    "                         error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 2 entries, Score to Text\n",
      "dtypes: object(1), float64(1)"
     ]
    }
   ],
   "source": [
    "Reviews_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=16</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>float64</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: from-delayed, 48 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                  Score    Text\n",
       "npartitions=16                 \n",
       "                float64  object\n",
       "                    ...     ...\n",
       "...                 ...     ...\n",
       "                    ...     ...\n",
       "                    ...     ...\n",
       "Dask Name: from-delayed, 48 tasks"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Reviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initially work with subsample of the data for rapid prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frac = 1.0\n",
    "# Reviews_df = Reviews_df.sample(frac=frac, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    363122\n",
       "4.0     80655\n",
       "1.0     52268\n",
       "3.0     42640\n",
       "2.0     29769\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Reviews_df['Score'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Reviews_df['Text']\n",
    "ylabels = Reviews_df['Score'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = np.unique(ylabels.compute())\n",
    "values = class_weight.compute_class_weight('balanced',\n",
    "                                           keys,\n",
    "                                           ylabels.compute())\n",
    "class_weights = dict(zip(keys, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.0: 2.1751511441034665,\n",
       " 2.0: 3.8191004064630993,\n",
       " 3.0: 2.666294559099437,\n",
       " 4.0: 1.4095939495381564,\n",
       " 5.0: 0.3130925694394721}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingVectorizer\n",
    "\n",
    "> This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.\n",
    "> This strategy has several advantages:\n",
    "* > it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in memory\n",
    "* > it is fast to pickle and un-pickle as it holds no state besides the constructor parameters\n",
    "* > it can be used in a streaming (partial fit) or parallel pipeline as there is no state computed during fit.\n",
    "\n",
    "> There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):\n",
    "* > there is no way to compute the inverse transform (from feature indices to string feature names) which can be a problem when trying to introspect which features are most important to a model.\n",
    "* > there can be collisions: distinct tokens can be mapped to the same feature index. However in practice this is rarely an issue if n_features is large enough (e.g. 2 ** 18 for text classification problems).\n",
    "* > no IDF weighting as this would render the transformer stateful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.feature_extraction.text import HashingVectorizer\n",
    "hw_vector = HashingVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1, 2), n_features=2**20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "Feature_pipeline = Pipeline([('vectorizer', hw_vector)])\n",
    "Pipeline_Model = Feature_pipeline.fit(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_preprocess_pipe = pickle.dumps(Pipeline_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipeline_Model = pickle.loads(Text_preprocess_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 8.34 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "X_transformed = Pipeline_Model.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Incremental' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Incremental' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import joblib\n",
    "estimator = SGDClassifier(random_state=10, max_iter=200, loss='modified_huber',class_weight = class_weights, n_jobs=-1)\n",
    "classifier = Incremental(estimator)\n",
    "Model = classifier.fit(X_transformed,\n",
    "               y_train,\n",
    "               classes=list(class_weights.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> unknown </td> <td> unknown </td></tr>\n",
       "    <tr><th> Shape </th><td> (nan,) </td> <td> (nan,) </td></tr>\n",
       "    <tr><th> Count </th><td> 128 Tasks </td><td> 16 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> int64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<_predict, shape=(nan,), dtype=int64, chunksize=(nan,), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = Model.predict(Pipeline_Model.transform(X_test))\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7197859176714592"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_Model = pickle.dumps(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "Model = pickle.loads(ML_Model)\n",
    "# X = Model.predict_proba(X_transformed).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "x_test_transformed = Pipeline_Model.transform(X_test)\n",
    "y_pred = Model.predict(x_test_transformed).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 9.06 µs\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.63      0.75      0.69     41872\n",
      "         2.0       0.39      0.60      0.47     23830\n",
      "         3.0       0.49      0.49      0.49     34064\n",
      "         4.0       0.59      0.34      0.43     64530\n",
      "         5.0       0.86      0.88      0.87    290370\n",
      "\n",
      "    accuracy                           0.75    454666\n",
      "   macro avg       0.59      0.61      0.59    454666\n",
      "weighted avg       0.75      0.75      0.74    454666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "print(classification_report(y_train,\n",
    "                            Model.predict(Pipeline_Model.transform(X_train)).compute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 9.54 µs\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.59      0.70      0.64     10396\n",
      "         2.0       0.32      0.49      0.38      5939\n",
      "         3.0       0.41      0.41      0.41      8576\n",
      "         4.0       0.53      0.30      0.39     16125\n",
      "         5.0       0.85      0.87      0.86     72752\n",
      "\n",
      "    accuracy                           0.72    113788\n",
      "   macro avg       0.54      0.55      0.54    113788\n",
      "weighted avg       0.72      0.72      0.71    113788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* https://examples.dask.org/machine-learning/text-vectorization.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
