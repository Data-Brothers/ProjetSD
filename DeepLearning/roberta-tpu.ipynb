{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n!pip install textblob pip install googletrans","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"### -------- Chargement des libraries ------- \nimport os\nimport tensorflow as tf\n\n# Ce dont nous avons besoin depuis tensorflow.keras\n\nfrom tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy,BinaryCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.utils import to_categorical\n\nfrom transformers import TFDistilBertModel, DistilBertConfig\n\n# Et pandas pour l'importation de donnees + sklearn pour le decoupage des donnees.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Instanciation TPU "},{"metadata":{"trusted":true},"cell_type":"code","source":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importation des données "},{"metadata":{"trusted":true},"cell_type":"code","source":"### --------- Importation et pretraitement des donées --------- \n## Importation des données\n\npath = '/kaggle/input/defi-ia-insa-toulouse/'\n\nnames=pd.read_csv(path+'categories_string.csv')['0'].to_dict()\n\ndf_X = pd.read_json(path + 'train.json')\ndf_label=pd.read_csv(path + 'train_label.csv')\n\ndata=pd.merge(df_X, df_label).drop(['Id'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fonctions utiles\n### Fairness (Disparate Impact)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def macro_disparate_impact(dataset,dico_jobs):\n    \"\"\"\n    Calcul le DI d'un dataset \n    dataset : au moins 2 colonnes (Job & Gender)\n    --------------------\n     Id job gender\n     0  1   F\n     1  7   M\n     2  1   M\n     3  23  M\n     4  23  M\n    \"\"\"\n    jobs = dataset.Category.map(names)\n    jobs = jobs.rename('job')\n    people=pd.concat((jobs,dataset.gender), axis='columns')\n    counts = people.groupby(['job', 'gender']).size().unstack('gender')\n    counts['disparate_impact'] = counts[[\"M\", \"F\"]].max(axis='columns') / counts[['M', 'F']].min(axis='columns')\n    return(counts['disparate_impact'],counts['disparate_impact'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DI_Global=macro_disparate_impact(data,names)\nprint(DI_Global[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Nettoyage de Texte"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport unidecode\ndef cleanText(string: str, punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''', stop_words=['the', 'a', 'and', 'is', 'be', 'will','on'])->str:\n    \"\"\" A method to clean text \"\"\"\n    string=unidecode.unidecode(string)\n    # Cleaning the urls\n    string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n    \n    # Nettoyage email\n    #string = re.sub(r'[at]', '@', string)\n    #string = re.sub(r'[dot]', '.', string)\n        \n    # Cleaning the urls\n    string = re.sub(r'\\[.*\\]', '', string)\n    \n    string = re.sub(r'\\]', '', string)\n    string = re.sub(r'\\[', '', string)\n    \n    string = re.sub(r'\\([A-Z]+\\)', '', string)\n    string = re.sub(r'\\([0-9]+\\)', '', string)\n    string = re.sub(r'[0-9]+', '#number', string)\n    string = re.sub(r'\\.+', '.', string)\n    \n    string = re.sub(r'Dr\\.', 'Doctor', string)\n    \n\n    # Removing the punctuations\n    #for x in string.lower(): \n    #    if x in punctuations: \n    #        string = string.replace(x, \"\") \n\n    # Converting the text to lower\n    #string = string.lower()\n\n    # Removing stop words\n    #string = ' '.join([word for word in string.split() if word not in stop_words])\n\n    # Cleaning the whitespaces\n    string = re.sub(r'\\s+', ' ', string).strip()\n\n    return string  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Augentation du dataset\n#### Calcul des classes à augmenter"},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_classe2augment(df):\n    \"\"\"renvoie la liste des classes à augmenter\"\"\"\n    # group by Category\n    gouped=df.groupby([\"Category\"]).count().sort_values(by=\"description\")#\n    gouped[\"name\"]=gouped.index.map(names)\n    return(gouped[gouped.description<=gouped.description.quantile(0.12)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes2augment=compute_classe2augment(data)\nprint(classes2augment)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Découpage des description en phrases"},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_desc(txt):\n    \"\"\"Découpe le texte en phrase\"\"\"\n    txt=str(txt)\n    #print(re.compile(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\").split(txt))\n    nptxt=np.array([[txt.strip()] for txt in re.compile(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\").split(txt)])\n    n=len(nptxt)\n    nptxt=nptxt.reshape(1, n)\n    return(nptxt[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Intervertion des phrases"},{"metadata":{"trusted":true},"cell_type":"code","source":"def augment_swap(txt):\n    # Découpage en phrase\n    array_txt=split_desc(txt)\n    # Assemblage random\n    np.random.shuffle(array_txt)\n    new_txt=''.join(array_txt)\n    return(new_txt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Swap Sentences\ndef swap_sentences(dataset,k_list,frac_new=0.5):\n    new_text=[]\n    for k in k_list:        \n        ## Selection des exemples à augmenter\n        df_n=dataset[dataset.Category==k].reset_index(drop=True).sample(frac=frac_new,replace=False)\n        ## data augmentation loop\n        for i in tqdm(range(0,len(df_n))):\n            single_desc = df_n.iloc[i]['description']\n            new_phrase = augment_swap(single_desc)\n            new_text.append({'description':new_phrase,'Category':k})\n    \n    return new_text","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"### TEST : Swap Sentences\n\n#df_count=compute_classe2augment(data)\n#new_desc_swap=swap_sentences(data,k_list=df_count.index,frac_new=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Augmentation en CrossOver "},{"metadata":{"trusted":true},"cell_type":"code","source":"def augment_crossover(desc_list,k=None):\n    \"\"\"Mélange des decription de la même classe\"\"\"\n    new_desc=''\n    for desc in desc_list:\n        desc= split_desc(desc)\n        desc=np.random.choice(desc,size=int(0.1*len(desc))+1, replace=False)\n        ## Selection aléatoire de la moitié de chaque  phrase \n        new_desc+=''.join(desc)    \n    return(new_desc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cross_over(dataset,k_list,num_new=10,frac_crossover=0.5):\n    new_text=[]\n    \n    for k in k_list:        \n        ## Selection des exemples à augmenter\n        df=dataset[dataset.Category==k].reset_index(drop=True)\n        \n        for num_new in tqdm(range(0,num_new)):\n            df_n=df.sample(frac=frac_crossover,replace=False)\n            new_phrase = augment_crossover(df_n.description)\n            new_text.append({'description':new_phrase,'Category':k})\n\n    return new_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### TEST : CrossOver\n\ndf_count=compute_classe2augment(train_data)\ndf_count.description=1000-df_count.description\n\nnew_desc=[]\nfor k in df_count.index:\n    num=df_count.description[k]\n    for i in range(num):\n        new_desc.append({'description':augment_crossover(train_data,k=k),'Category':k})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Augmentation par Traduction"},{"metadata":{"trusted":true},"cell_type":"code","source":"from deep_translator import GoogleTranslator, MyMemoryTranslator\nfrom textblob import TextBlob\n\ndef translate_en_fr_deeptrans(desc_list):\n    \n    translator_chinese= MyMemoryTranslator(source='auto', target='chinese (simplified)')\n    translator_fr= MyMemoryTranslator(source='auto', target='fr')\n    translator_en= MyMemoryTranslator(source='auto', target='en')\n    translated_list= translator_chinese.translate_batch(desc_list)\n    sleep(10)\n    #translated_list = translator_en.translate_batch()\n    return(translated_list)\n\ndef translate_en_fr_TextBlob(desc):\n    blob=TextBlob(desc)\n    blob=blob.translate(to='zh-CN')\n    blob=blob.translate(to='en')\n     #translated_list = translator_en.translate_batch()\n    return(blob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def DoubleTranslation_DeepTrans(dataset,k_list,frac_new=0.5):\n    new_text=[]\n    for k in k_list:\n        ## Selection des exemples à augmenter\n        df_n=dataset[dataset.Category==k].reset_index(drop=True).sample(frac=frac_new,replace=False)\n        ## data augmentation loop\n        new_phrase = translate_en_fr_deeptrans(df_n.description.to_list())\n        print(new_phrase)\n        #new_text.append({'description':new_phrase,'Category':k})\n    return new_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Swap Sentences\ndef DoubleTranslation_Textblob(dataset,k_list,frac_new=0.5):\n    new_text=[]\n    for k in k_list:        \n        ## Selection des exemples à augmenter\n        df_n=dataset[dataset.Category==k].reset_index(drop=True).sample(frac=frac_new,replace=False)\n        ## data augmentation loop\n        for i in tqdm(range(0,len(df_n))):\n            sleep(1)\n            single_desc = df_n.iloc[i]['description']\n            new_phrase = translate_en_fr_TextBlob(single_desc)\n            new_text.append({'description':new_phrase,'Category':k})\n    \n    return new_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.iloc[0]['description'])\nprint(\"\\n\\n\")\nprint(translate_en_fr_TextBlob((data.iloc[0]['description'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['gender'] = pd.Categorical(data['gender'])\ndata['Category'] = pd.Categorical(data['Category'])\ndata['description']=data.description.apply(lambda x: cleanText(x))\n# Découpage Train - Validation \ntrain_data, valid_data = train_test_split(data, test_size = 0.4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Augment_Swap\ndf_count=compute_classe2augment(data)\nnew_desc_swap=swap_sentences(new_train_data,k_list=df_count.index,frac_new=0.2)\nnew_train_data=pd.concat([new_train_data,pd.DataFrame(new_desc_swap)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Augment_CrossOver\ndf_count=compute_classe2augment(data)\nnew_desc_cross_over=cross_over(train_data,df_count.index,num_new=100,frac_crossover=0.7)\nnew_train_data=pd.concat([train_data,pd.DataFrame(new_desc_cross_over)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Augment_Translation\ndf_count=compute_classe2augment(data)\nnew_desc_translated=DoubleTranslation_Textblob(train_data,df_count.index,frac_new=0.2)\n#new_train_data=pd.concat([train_data,pd.DataFrame(new_desc_translated)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.shape)\n#print(new_train_data.shape)\n\n\nprint(valid_data.shape)\n\nprint(train_data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Instanciation du modèle"},{"metadata":{"trusted":true},"cell_type":"code","source":"### --------- Setup BERT ---------- #\nfrom transformers import DistilBertTokenizer, RobertaTokenizer,DistilBertConfig,TFRobertaModel,RobertaConfig\n\nMAX_LENGTH=300\ndistil_bert = 'distilbert-base-cased' # Pick any desired pre-trained model\nroberta = 'roberta-large'\n\n# Defining DistilBERT tokonizer\nDistiltokenizer = DistilBertTokenizer.from_pretrained(distil_bert, do_lower_case=True, add_special_tokens=True,max_length=MAX_LENGTH, pad_to_max_length=True)\nRobTokenizer=RobertaTokenizer.from_pretrained(roberta, do_lower_case=True, add_special_tokens=True,max_length=MAX_LENGTH, pad_to_max_length=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(sentences, tokenizer):\n    input_ids, input_masks, input_segments = [],[],[]\n    for sentence in tqdm(sentences):\n        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=MAX_LENGTH, pad_to_max_length=True, \n                                             return_attention_mask=True, return_token_type_ids=True)\n        input_ids.append(inputs['input_ids'])\n        input_masks.append(inputs['attention_mask'])\n        input_segments.append(inputs['token_type_ids'])        \n        \n    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###############\n## Autre Model\n###############\n\ndef create_model_bis(BertModel='distilbert-base-uncased',max_length=MAX_LENGTH):\n  \n    transformer_model = TFRobertaModel.from_pretrained(BertModel)\n    \n    # Couche d'entrée du modèle\n    input_ids_in = Input(shape=(max_length,), name='input_token', dtype='int32')\n    input_masks_in = Input(shape=(max_length,), name='masked_token', dtype='int32') \n\n    hidden_states = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\n    cls_token = hidden_states[:,0,:]\n    \n    \n    X= tf.keras.layers.BatchNormalization()(cls_token)\n    \n    X = tf.keras.layers.Dense(256, activation='sigmoid')(X)\n    X = tf.keras.layers.Dropout(0.3)(X)\n    X = tf.keras.layers.Dense(128, activation='sigmoid')(X)\n    X = tf.keras.layers.Dropout(0.3)(X)\n    X = tf.keras.layers.Dense(28, activation='softmax')(X)\n    \n    model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model_bis(BertModel='distilbert-base-uncased',max_length=300):\n  \n    transformer= TFRobertaModel.from_pretrained(BertModel, output_hidden_states=True)\n    \n    # Couche d'entrée du modèle\n    input_ids_in = Input(shape=(max_length,), name='input_token', dtype='int32')\n    input_masks_in = Input(shape=(max_length,), name='masked_token', dtype='int32') \n\n    hidden_states = transformer(input_ids_in,attention_mask=input_masks_in)[2]#, \n\n    merged = tf.keras.layers.concatenate(tuple([hidden_states[i] for i in [-3,-2, -1]]))\n    X= tf.keras.layers.BatchNormalization()(X)\n    X= tf.keras.layers.BatchNormalization()()\n    \n    X=tf.keras.layers.Conv1D(filters=6, kernel_size=4,strides=3,padding='valid', activation='relu')(merged)\n    X=tf.keras.layers.Conv1D(filters=6, kernel_size=10,strides=3,padding='valid', activation='relu')(merged)\n    \n    X=tf.keras.layers.MaxPool1D(pool_size=2)(X)\n    X=tf.keras.layers.Conv1D(filters=6, kernel_size=4,strides=1,padding='valid', activation='relu')(X)\n    X=tf.keras.layers.MaxPool1D(pool_size=2)(X)\n    X=tf.keras.layers.Conv1D(filters=128, kernel_size=4,strides=1,padding='valid', activation='relu')(X)\n    X = tf.keras.layers.Flatten()(X)\n    \n    X = tf.keras.layers.Dense(32,activation='relu')(X)\n    #X = tf.keras.layers.Dense(128, activation='relu')(X)\n    X = tf.keras.layers.Dropout(0.2)(X)\n    X = tf.keras.layers.Dense(28, activation='softmax')(X)\n    model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)#\n\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"new_train_data=new_train_data.sample(frac=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenization / Creation des inputs : TrainSet\ninput_ids,input_masks,input_segments=tokenize(train_data.description, RobTokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenization / Creation des inputs : ValidationSet\nvalid_input_ids,valid_input_masks,valid_input_segments=tokenize(valid_data.description, RobTokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#########################\n### Outils pour le réseau\n#########################\nfrom tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\nloss = tf.keras.losses.SparseCategoricalCrossentropy()\nearlyStopping=EarlyStopping(monitor='val_f1',patience=5, verbose=1,mode='max')\n\nfilepath=\"Model_{epoch:02d}_{val_f1}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_f1', verbose=1, save_weights_only=True,save_best_only=True,mode='max')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Important pour le TPU\nwith tpu_strategy.scope():\n    model = create_model_bis(BertModel=roberta)\n    model.summary()\n    model.compile(optimizer=optimizer,loss=loss,metrics=['acc']) # Ca ne sert à rien de mettre le F1 Score ici","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score, recall_score, precision_score\n\nclass Metrics(tf.keras.callbacks.Callback):\n    def __init__(self, valid_data):\n        super(Metrics, self).__init__()\n        self.validation_data = valid_data\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        val_predict = np.argmax(self.model.predict(self.validation_data[0]), -1)\n        val_targ = self.validation_data[1]\n        if len(val_targ.shape) == 2 and val_targ.shape[1] != 1:\n            val_targ = np.argmax(val_targ, -1)\n\n        _val_f1 = f1_score(val_targ, val_predict, average='macro',zero_division=0)\n        _val_recall = recall_score(val_targ, val_predict, average='macro',zero_division=0)\n        _val_precision = precision_score(val_targ, val_predict, average='macro',zero_division=0)\n\n        logs['val_f1'] = _val_f1\n        logs['val_recall'] = _val_recall\n        logs['val_precision'] = _val_precision\n        print(\" — val_f1: %f — val_precision: %f — val_recall: %f\" % (_val_f1, _val_precision, _val_recall))\n        return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###########################\n### Apprentissage du Réseau\n###########################\nBATCH_SIZE=64\nEPOCHS=50\nhistory=model.fit([input_ids,input_masks],train_data.Category,\n                  batch_size=BATCH_SIZE,epochs=EPOCHS,\n                  callbacks=[Metrics(valid_data=([valid_input_ids,valid_input_masks],valid_data.Category)),earlyStopping,checkpoint],\n                  verbose=1,shuffle=True)\n\n## Pour le model 3 UNIQUEMENT\n## model.fit([input_ids,input_masks],y={'gender': train_data.gender, 'jobs': train_data.Category},verbose=2,validation_split=0.3,batch_size=BATCH_SIZE,epochs=EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\ndef Load_Model(path):\n    new_model = create_model_bis(BertModel=roberta)\n    new_model.load_weights(path)\n    return(new_model)\n\ndef predict_classes(new_model,valid_inputs):\n    proba=new_model.predict(valid_inputs)\n    classes=proba.argmax(axis=-1)\n    return(classes)\n\ndef inspect_results(y_pred,y_true):\n    # Voir les prédictions ou il se trompe\n    # Regarder dans quelle classe il classe mal ?\n    # Quelles têtes ont les descriptions ? \n    ### Important pour le TPU\n    #y_pred[\"name\"]=y_pred.map(names)\n    print(f1_score(y_pred,y_true))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_model=Load_Model(path=\"./Model_08_0.7986206856061193.hdf5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes=predict_classes(new_model,[valid_input_ids,valid_input_masks])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### --------- Importation et pretraitement des donées --------- \n## Importation des données\n\npath = '/kaggle/input/defi-ia-insa-toulouse/'\ndf_X_test = pd.read_json(path + 'test.json')\n\ndata_test=df_X_test.drop(['Id'], axis = 1)\n\ndata_test['gender'] = pd.Categorical(data_test['gender'].replace({'M': 0, 'F': 1}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids_test,input_masks_test,input_segments_test=tokenize(data_test.description, RobTokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"proba_test=model.predict([input_ids_test,input_masks_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classesproba_test=proba_test.argmax(axis=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classesproba_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submis=pd.DataFrame(classesproba_test,columns =['Category'])\nprint(submis.shape)\nprint(submis.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submis.to_csv('sub031220')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}